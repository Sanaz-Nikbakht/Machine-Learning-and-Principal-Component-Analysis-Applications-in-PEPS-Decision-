In:
 import pandas as pd

data = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set.xlsx")

# Read Excel file into a DataFrame
df = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set.xlsx")

# Display the first few rows of the DataFrame
print(df.head(13))

out:
   Engine Oil Level Sensor  Driver Weight Sensor(kg)  \
0                         2                        65   
1                         1                        53   
2                         1                        68   
3                         2                        82   
4                         2                        76   
5                         2                         0   
6                         2                        73   
7                         1                        74   
8                         2                         0   
9                         1                        74   
10                        1                        74   
11                        1                        62   
12                        2                         0   
                           PBP(psi)  Automobile Speed Sensor(rev/m)  \
0                            800                               0   
1                            850                            3000   
2                            830                            4000   
3                            840                               0   
4                            910                               0   
5                              0                               0   
6                            852                            2700   
7                            874                            4100   
8                              0                               0   
9                           1000                               0   
10                          1100                               0   
11                           963                               0   
12                             0                               0   
    12 V Electricity Power   PEPS Decision  
0                      12.0              1  
1                      12.0              0  
2                      11.9              0  
3                      12.0              1  
4                      12.0              1  
5                       0.0              0  
6                      12.0              0  
7                      11.8              0  
8                       0.0              0  
9                       0.0              1  
10                     12.0              1  
11                     12.0              1  
12                     11.7              0  
In:
print(df.shape)
 Out:
(13, 6)





















In:
df.info()
Out:
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 13 entries, 0 to 12
Data columns (total 6 columns):
 #   Column                          Non-Null Count  Dtype  
---  ------                          --------------  -----  
 0   Engine Oil Level Sensor         13 non-null     object 
 1   Driver Weight Sensor(kg)        13 non-null     int64  
 2   PBP(psi)                                      13 non-null     int64  
 3   Automobile Speed Sensor(rev/m)  13 non-null     int64  
 4   12 V Electricity Power          13 non-null     float64
 5   PEPS Decision                   13 non-null     int64  
dtypes: float64(1), int64(4), object(1)
memory usage: 756.0+ bytes









In:
print(df.describe(include='all'))
Out:
      Engine Oil Level Sensor  Driver Weight Sensor(kg)  \
count                13.000000                 13.000000   
mean                  1.538462                 53.923077   
std                   0.518875                 31.568607   
min                   1.000000                  0.000000   
25%                   1.000000                 53.000000   
50%                   2.000000                 68.000000   
75%                   2.000000                 74.000000   
max                   2.000000                 82.000000   
                                PBP(psi)  Automobile Speed Sensor(rev/m)  \
count                     13.000000                       13.000000   
mean                     693.769231                     1061.538462   
std                      403.644265                     1694.372284   
min                        0.000000                        0.000000   
25%                      800.000000                        0.000000   
50%                      850.000000                        0.000000   
75%                      910.000000                     2700.000000   
max                     1100.000000                     4100.000000   
       12 V Electricity Power   PEPS Decision  
count                13.000000      13.000000  
mean                  9.184615       0.461538  
std                   5.236864       0.518875  
min                   0.000000       0.000000  
25%                  11.700000       0.000000  
50%                  12.000000       0.000000  
75%                  12.000000       1.000000  
max                  12.000000       1.000000  






	













In:
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
data = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set.xlsx")

# Read Excel file into a DataFrame
df = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set.xlsx")
sns.pairplot(df, hue='PEPS Decision')
plt.show()
 Out:
 








Data standardization
In:
from sklearn.preprocessing import (StandardScaler)
import pandas as pd

#Assuming 'X' is your DataFrame
pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1.xlsx")
df = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1.xlsx")

# Extract the numerical columns you want to standardize
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns

# Create a StandardScaler object
scaler = StandardScaler()

# Fit the scaler on the data and transform the numerical columns
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

# Display the standardized DataFrame
pd.set_option('display.max_columns', None)
print(df)
 Out:
  Engine Oil Level  Driver Weight(kg)  PBP(psi)  Car Speed(rev/m)  \
0           0.925820           0.365212  0.273926         -0.652091   
1          -1.080123          -0.030434  0.402855          1.190774   
2          -1.080123           0.464123  0.351283          1.805062   
3           0.925820           0.925710  0.377069         -0.652091   
4           0.925820           0.727887  0.557570         -0.652091   
5           0.925820          -1.777871 -1.788946         -0.652091   
6           0.925820           0.628976  0.408012          1.006488   
7          -1.080123           0.661946  0.464741          1.866491   
8           0.925820          -1.777871 -1.788946         -0.652091   
9          -1.080123           0.661946  0.789644         -0.652091   
10         -1.080123           0.661946  1.047503         -0.652091   
11         -1.080123           0.266300  0.694236         -0.652091   
12          0.925820          -1.777871 -1.788946         -0.652091   

    12 V Electricity Power   
0                  0.559561  
1                  0.559561  
2                  0.539686  
3                  0.559561  
4                  0.559561  
5                 -1.825453  
6                  0.559561  
7                  0.519811  
8                 -1.825453  
9                 -1.825453  
10                 0.559561  
11                 0.559561  
12                 0.499936  






BoX Plot
In:
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
#Assuming 'X' is your DataFrame
pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1.xlsx")
df = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1.xlsx")
# Use swarmplot() or stripplot to show the datapoints on top of the boxes:
#plt. figure()
# set the figure size
ax = plt.figure(figsize=(15,10))
# create the boxplot with seaborn
ax = sns.boxplot(data=X, orient="v", palette="Set2")
# overlay the datapoints with stripplot or swarmplot
ax = sns.stripplot(data=X, color=".25")
# rotate the x-axis tick labels
ax.set_xticklabels(ax.get_xticklabels(),rotation=45)
plt.show()	
 Out:

 
Violin Plot
In:
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
#Assuming 'X' is your DataFrame
pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1.xlsx")
df = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1.xlsx")
plt.figure(figsize=(15,10))
sns.violinplot(data=X, palette="Set2")
plt.xticks(rotation=45)
plt.show()
 Out:


 



Correlation Matrix
In:
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
#Assuming 'X' is your DataFrame
pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1.xlsx")
df = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1.xlsx")
plt.figure(figsize=(15,10))
fig1,ax1 = plt.subplots(figsize=(15,10))
ax = sns.heatmap(X.corr(), cmap='RdYlGn_r', linewidths=0.3, annot=True, cbar=False, square=True)
plt.yticks(rotation=0)
ax.tick_params(labelbottom=False,labeltop=True)
ax.set_xticklabels(ax.get_xticklabels(),rotation=0);
plt.show()

#X.corr().style.background_gradient(cmap='coolwarm').set_precision(2)
#sns.clustermap(X.corr(), annot=True, fmt='.2f')
 Out:
 



Principal Component Analysis (PCA)
In:
mport numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

#Assuming 'X' is your DataFrame
pd.read_excel(r"E:\Sanaz Assignment\Python\data_set.xlsx")
X = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set.xlsx")


# Standardize the data
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Apply PCA
pca = PCA()
X_pca = pca.fit_transform(X_standardized)

# 'X_pca' now contains the transformed data with reduced dimensionality

# Display the principal components (optional)
principal_components = pd.DataFrame(data=X_pca, columns=[f'PC{i+1}' for i in range(X_pca.shape[1])])
print(principal_components)
Out:
         PC1                   PC2           PC3                PC4               PC5
0   0.013969     1.303738    -0.297577    -0.104945     -0.066292
1  -1.240813    -1.101651    -0.383413     0.353822    -0.188384
2  -1.668789    -1.363232    -0.620234    -0.128553     0.102537
3  -0.340167     1.508430   -0.173883    -0.356909      0.214244
4  -0.334066     1.487317    -0.153766    -0.294689    -0.044514
5   3.221756    -0.665268     0.287186    -0.265559    -0.022569
6  -0.743569     0.334906    -1.069963   -0.953686    -0.161119
7  -1.848334    -1.319630    -0.572714    -0.264701     0.137795
8   3.221756    -0.665268     0.287186    -0.265559    -0.022569
9  -0.276790    -0.374785    2.344751    -0.519226     0.036729
10 -1.357339   0.479046     0.872291     0.778477    -0.040727
11 -0.953637   0.268445     0.702518     0.990869    -0.032201
12  2.306023   0.107953    -1.222382    1.030660     0.087071


















Scree plot
In:
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

#Assuming 'X' is your DataFrame
pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1.xlsx")
X = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1.xlsx")


# Standardize the data
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Apply PCA
pca = PCA()
X_pca = pca.fit_transform(X_standardized)

# 'X_pca' now contains the transformed data with reduced dimensionality

# Display the principal components (optional)
principal_components = pd.DataFrame(data=X_pca, columns=[f'PC{i+1}' for i in range(X_pca.shape[1])])
#Eigenvalues
Lambda = pca.explained_variance_ 

#Scree plot
x = np.arange(len(Lambda)) + 1
plt. figure(figsize=(15,10))
plt.plot(x,Lambda/sum(Lambda), 'ro-', lw=3)
plt.xticks(x, [""+str(i) for i in x], rotation=0)
plt.xlabel('Number of components')
plt.ylabel('Explained variance')
Lambda

plt.show()
Out:
 





	





Pareto plot
In:
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

#Assuming 'X' is your DataFrame
pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1.xlsx")
X = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1.xlsx")


# Standardize the data
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Apply PCA
pca = PCA()
X_pca = pca.fit_transform(X_standardized)

# 'X_pca' now contains the transformed data with reduced dimensionality

# Display the principal components (optional)
principal_components = pd.DataFrame(data=X_pca, columns=[f'PC{i+1}' for i in range(X_pca.shape[1])])
ell = pca.explained_variance_ratio_
ind = np.arange(len(ell))
plt. figure(figsize=(15,10))
plt.bar(ind, ell, align='center', alpha=0.5)
plt.plot(np.cumsum(ell))
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')

plt.show()








Out:
 



	











Pareto Plot Table
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

#Assuming 'X' is your DataFrame
pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1.xlsx")
X = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1.xlsx")


# Standardize the data
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Apply PCA
pca = PCA()
X_pca = pca.fit_transform(X_standardized)

# 'X_pca' now contains the transformed data with reduced dimensionality

# Display the principal components (optional)
principal_components = pd.DataFrame(data=X_pca, columns=[f'PC{i+1}' for i in range(X_pca.shape[1])])

from tabulate import tabulate

ell = pca.explained_variance_ratio_
cum_ell = np.cumsum(ell)

# create a table with the data
table_data = []
for i in range(len(ell)):
    table_data.append([i+1, ell[i], cum_ell[i]])

table_header = ['Component', 'Explained Variance', 'Cumulative Explained Variance']

# print the table
print(tabulate(table_data, headers=table_header, tablefmt='grid'))

Out:
+-------------+----------------------+---------------------------------+
|   Component |   Explained Variance |   Cumulative Explained Variance |
+=============+======================+=====================+
|              1 |                 0.575797   |                             0.575797 |
+-------------+----------------------+---------------------------------+
|           2 |                   0.191564   |                              0.767361 |
+-------------+----------------------+---------------------------------+
|           3 |                   0.162437   |                        0.929798 |
+-------------+----------------------+---------------------------------+
|           4 |                0.0678044  |                        0.997602 |
+-------------+----------------------+---------------------------------+
|           5 |                  0.00239801 |                        1        |
+-------------+----------------------+---------------------------------+








PC Coefficient Plot
In:
import pandas as pd
from sklearn.decomposition import PCA
import numpy as np

excel_file_path = r"E:\Sanaz Assignment\Python\data_set, Box Plot.xlsx"
data = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set, Box Plot.xlsx")

# Create a PCA instance
pca = PCA(n_components=2)  # You can specify the number of principal components you want

# Fit and transform the data
principal_components = pca.fit_transform(data)

# Accessing the principal components
pc1 = principal_components[:, 0]
pc2 = principal_components[:, 1]
# Print the principal components
print("Principal Component 1:", pc1)
print("Principal Component 2:", pc2)

Out:
Principal Component 1: [ 0.01396866 -1.24081314 -1.6687886  -0.34016694 -0.33406561  3.22175549
 -0.74356894 -1.84833354  3.22175549 -0.27679039 -1.35733858 -0.95363687
  2.30602297]
Principal Component 2: [ 1.30373774 -1.10165181 -1.36323302  1.50842936  1.48731607 -0.66526769
  0.33490347 -1.31963081 -0.66526769 -0.37478137  0.47904811  0.26844693
  0.10795071]



Eigenvectors

In:
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from pca import pca

excel_file_path = r"E:\Sanaz Assignment\Python\data_set1, Coefficient Plot.xlsx"
df = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1, Coefficient Plot.xlsx")

# Extract data from the DataFrame
loadings = df[['Car Speed(rev/m)', '12 V Electricity Power ']].values
principal_components = df[['PC1', 'PC2']].values

# Transpose of 'loadings' matrix
A = loadings.T

# Matrix multiplication: A = out['loadings'].T * principal_components
result_A = np.dot(A, principal_components)

# Print the result
print(result_A)
Out:
[[-4.81757885e+03  1.13667991e+03]
 [ 1.96820835e+05 -1.34796253e+05]
 [ 2.30607112e+06 -1.81050312e+06]]




PC Coefficient Plot

In:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# Load data from Excel file
excel_file_path = r"E:\Sanaz Assignment\Python\data_set1, Coefficient Plot1.xlsx"
A = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1, Coefficient Plot1.xlsx")
# Scatter plot
sns.scatterplot(data=A, x="PC1", y="PC2")
plt.xlabel('A1')
plt.ylabel('A2')
# Annotate points
for i in range(A.shape[0]):
    plt.text(x=A.PC1[i] + 0.004, y=A.PC2[i] + 0.005, s=A.variables[i],
             fontdict=dict(color='red', size=10),
             bbox=dict(facecolor='yellow', alpha=0.5))
# Show the plot
plt.show()
Out:
 
Scree plot after pca, Explained Variance

In:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
# Load data from Excel file
excel_file_path = r"E:\Sanaz Assignment\Python\data_set1, Coefficient Plot1.xlsx"  # Replace with the actual path to your Excel file
df = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1, Coefficient Plot1.xlsx")
# Assuming your data is numeric, you can perform PCA
# Extract numeric columns (you may need to adjust this based on your actual data)
numeric_columns = df.select_dtypes(include=[np.number]).columns
data = df[numeric_columns]
# Perform PCA
pca = PCA()
pca.fit(data)
# Get the explained variance ratios
variance_ratio = pca.explained_variance_ratio_
# Plotting the scree plot
components = np.arange(1, len(variance_ratio) + 1)
plt.plot(components, variance_ratio, 'ro-', lw=3)
plt.xlabel('Number of Components')
plt.ylabel('Explained Variance Ratio')
plt.title('Scree Plot')
plt.show()
Out:
 











In:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
# Load data from Excel file
excel_file_path = r"E:\Sanaz Assignment\Python\data_set1, Coefficient Plot1.xlsx"  # Replace with the actual path to your Excel file
df = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set1, Coefficient Plot1.xlsx")
# Splitting the data into training and unseen data
data = df.sample(frac=0.8, random_state=786)
data_unseen  df.drop(data.index)
# Resetting the index for both datasets
data.reset_index(drop=True, inplace=True)
data_unseen.reset_index(drop=True, inplace=True)
# Printing the shapes of the datasets
print('Data for Modeling: ' + str(data.shape))
print('Unseen Data For Predictions: ' + str(data_unseen.shape))

Out:
Data for Modeling: (4, 3)
Unseen Data For Predictions: (1, 3)





In:
import pandas as pd
from pycaret.classification import *
excel_file_path = r"E:\Sanaz Assignment\Python\data_set.xlsx"
data = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set.xlsx")
# Replace 'Occupancy' with your target column name
clf = setup(data=data, target='PEPS Decision',   train_size=0.8, session_id=123)
Out:
                    Description                Value
0                    Session id                  123
1                        Target             PEPS Decision
2                   Target type               Binary
3           Original data shape           (13, 6)
4        Transformed data shape           (13, 6)
5   Transformed train set shape           (10, 6)
6    Transformed test set shape            (3, 6)
7              Numeric features                 5
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              5016







	


















Comparing Models before PCA

In:
import pandas as pd
from pycaret.classification import *
# Load your dataset
data = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set.xlsx")
# Set up the classification environment
clf = setup(data=data, target='PEPS Decision', train_size=0.8, session_id=123)
# Now you can create a model
dt = create_model('dt', fold=5)
# Or you can compare models
compare_models(fold=5)

Out:
                                   Model  Accuracy  AUC  Recall  Prec.  \
lr                    Logistic Regression       0.9  1.0     1.0    0.9   
knn                K Neighbors Classifier       0.9  0.9     1.0    0.9   
svm                   SVM - Linear Kernel       0.9  0.0     1.0    0.9   
ridge                    Ridge Classifier       0.9  0.0     1.0    0.9   
et                 Extra Trees Classifier       0.9  1.0     1.0    0.9   
nb                            Naive Bayes       0.8  0.7     0.8    0.7   
rf               Random Forest Classifier       0.8  1.0     0.8    0.7   
lda          Linear Discriminant Analysis       0.8  0.8     0.8    0.7   
dt               Decision Tree Classifier       0.7  0.7     0.6    0.5   
qda       Quadratic Discriminant Analysis       0.7  0.6     0.8    0.6   
ada                  Ada Boost Classifier       0.7  0.8     0.8    0.6   
gbc          Gradient Boosting Classifier       0.7  0.8     0.6    0.5   
lightgbm  Light Gradient Boosting Machine       0.5  0.5     0.0    0.0   
dummy                    Dummy Classifier       0.5  0.5     0.0    0.0   

              F1  Kappa  MCC  TT (Sec)  
lr        0.9333    0.8  0.8     0.100  
knn       0.9333    0.8  0.8     0.240  
svm       0.9333    0.8  0.8     0.032  
ridge     0.9333    0.8  0.8     0.032  
et        0.9333    0.8  0.8     0.192  
nb        0.7333    0.6  0.6     0.040  
rf        0.7333    0.6  0.6     0.254  
lda       0.7333    0.6  0.6     0.028  
dt        0.5333    0.4  0.4     0.032  
qda       0.6667    0.4  0.4     0.032  
ada       0.6667    0.4  0.4     0.078  
gbc       0.5333    0.4  0.4     0.098  
lightgbm  0.0000    0.0  0.0     0.270  
dummy     0.0000    0.0  0.0     0.028  


Comparing Models after applying PCA

In:
import pandas as pd
from pycaret.classification import *
# Load your dataset
data = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set.xlsx")
# Set up the classification environment with PCA
clf = setup(data=data, target='PEPS Decision', train_size=0.8, pca=True, pca_components=5, session_id=123)
# Create a model after applying PCA
dt_after_pca = create_model('dt', fold=5)
# Compare models
compare_models(fold=5)

Out:
                                  Model  Accuracy  AUC  Recall  Prec.  \
lr                    Logistic Regression       0.9  1.0     1.0    0.9   
knn                K Neighbors Classifier       0.9  0.9     1.0    0.9   
nb                            Naive Bayes       0.9  0.8     1.0    0.9   
svm                   SVM - Linear Kernel       0.9  0.0     1.0    0.9   
ridge                    Ridge Classifier       0.9  0.0     1.0    0.9   
rf               Random Forest Classifier       0.9  1.0     1.0    0.9   
et                 Extra Trees Classifier       0.9  1.0     1.0    0.9   
dt               Decision Tree Classifier       0.8  0.8     0.8    0.7   
qda       Quadratic Discriminant Analysis       0.8  0.8     1.0    0.8   
ada                  Ada Boost Classifier       0.8  0.8     0.8    0.7   
gbc          Gradient Boosting Classifier       0.8  0.8     0.8    0.7   
lda          Linear Discriminant Analysis       0.8  0.8     0.8    0.7   
lightgbm  Light Gradient Boosting Machine       0.5  0.5     0.0    0.0   
dummy                    Dummy Classifier       0.5  0.5     0.0    0.0   

              F1  Kappa  MCC  TT (Sec)  
lr        0.9333    0.8  0.8     0.066  
knn       0.9333    0.8  0.8     0.098  
nb        0.9333    0.8  0.8     0.038  
svm       0.9333    0.8  0.8     0.050  
ridge     0.9333    0.8  0.8     0.098  
rf        0.9333    0.8  0.8     0.242  
et        0.9333    0.8  0.8     0.188  
dt        0.7333    0.6  0.6     0.038  
qda       0.8667    0.6  0.6     0.062  
ada       0.7333    0.6  0.6     0.046  
gbc       0.7333    0.6  0.6     0.086  
lda       0.7333    0.6  0.6     0.054  
lightgbm  0.0000    0.0  0.0     0.078  
dummy     0.0000    0.0  0.0     0.036
Create a Model
Logistic Regression       
In:
import pandas as pd
from pycaret.classification import *
# Load your dataset
data = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set.xlsx")
# Set up the classification environment
clf = setup(data=data, target='PEPS Decision', train_size=0.8, session_id=123)
lr = create_model('lr', fold=5)

Out:
      Accuracy  AUC  Recall  Prec.      F1  Kappa  MCC
Fold                                                  
0          1.0  1.0     1.0    1.0  1.0000    1.0  1.0
1          0.5  1.0     1.0    0.5  0.6667    0.0  0.0
2          1.0  1.0     1.0    1.0  1.0000    1.0  1.0
3          1.0  1.0     1.0    1.0  1.0000    1.0  1.0
4          1.0  1.0     1.0    1.0  1.0000    1.0  1.0
Mean       0.9  1.0     1.0    0.9  0.9333    0.8  0.8
Std        0.2  0.0     0.0    0.2  0.1333    0.4  0.4




Tuning a Model
Logistic Regression       
In:
# Load your dataset
data = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set.xlsx")
# Set up the classification environment
clf = setup(data=data, target='PEPS Decision', train_size=0.8, session_id=123)
# Create and train the Logistic Regression model
lr = create_model('lr', fold=5)  # Use fold=5 instead of fold=10
# Tune the Logistic Regression model
tuned_lr = tune_model(lr, fold=5)
Out:
      Accuracy  AUC  Recall  Prec.      F1  Kappa  MCC
Fold                                                  
0          1.0  1.0     1.0    1.0  1.0000    1.0  1.0
1          0.5  1.0     1.0    0.5  0.6667    0.0  0.0
2          1.0  1.0     1.0    1.0  1.0000    1.0  1.0
3          1.0  1.0     1.0    1.0  1.0000    1.0  1.0
4          1.0  1.0     1.0    1.0  1.0000    1.0  1.0
Mean       0.9  1.0     1.0    0.9  0.9333    0.8  0.8
Std        0.2  0.0     0.0    0.2  0.1333    0.4  0.4




Tuning a Model
K Neighbors Classifier
In:
import pandas as pd
from pycaret.classification import *
# Load your dataset
data = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set.xlsx")
# Set up the classification environment
clf = setup(data=data, target='PEPS Decision', train_size=0.8, session_id=123)
# Create and train the K Neighbors Classifier model
knn = create_model('knn', fold=5)  # Use fold=5 instead of fold=10
# Tune the K Neighbors Classifier model
tuned_knn = tune_model(knn, fold=5)
Out:
      Accuracy  AUC  Recall  Prec.      F1  Kappa  MCC
Fold                                                  
0          1.0  1.0     1.0    1.0  1.0000    1.0  1.0
1          0.5  0.5     1.0    0.5  0.6667    0.0  0.0
2          1.0  1.0     1.0    1.0  1.0000    1.0  1.0
3          1.0  1.0     1.0    1.0  1.0000    1.0  1.0
4          1.0  1.0     1.0    1.0  1.0000    1.0  1.0
Mean       0.9  0.9     1.0    0.9  0.9333    0.8  0.8
Std        0.2  0.2     0.0    0.2  0.1333    0.4  0.4






Tuning a Model
Naive Bayes
In:
import pandas as pd
from pycaret.classification import *
# Load your dataset
data = pd.read_excel(r"E:\Sanaz Assignment\Python\data_set.xlsx")
# Set up the classification environment
clf = setup(data=data, target='PEPS Decision', train_size=0.8, session_id=123)
# Create and train the Naive Bayes model
nb = create_model('nb', fold=5)
# Optionally, you can also tune the Naive Bayes model if needed
tuned_nb = tune_model(nb, fold=5)

Out:
      Accuracy  AUC  Recall  Prec.      F1  Kappa  MCC
Fold                                                  
0          1.0  1.0     1.0    1.0  1.0000    1.0  1.0
1          0.5  0.0     1.0    0.5  0.6667    0.0  0.0
2          1.0  1.0     1.0    1.0  1.0000    1.0  1.0
3          1.0  1.0     1.0    1.0  1.0000    1.0  1.0
4          1.0  1.0     1.0    1.0  1.0000    1.0  1.0
Mean       0.9  0.8     1.0    0.9  0.9333    0.8  0.8
Std        0.2  0.4     0.0    0.2  0.1333    0.4  0.4
